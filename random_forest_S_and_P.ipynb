# Model Selection under RFECV
# Intro
# In this file, we trained and tested 5 different classifiers on  the same given data from S&P:
# Logistic Regression
# Random Forest
# K Nearest Neighbors
# Support Vector Machine
# Decision Tree

# For each model, 2 things need to be aware of:

# We used RFECV (Recursive Feature Elimination + Cross Validation) for feature selection. Instead of we deciding the number of features to be selected by RFE, we added cross validate to decide this crucial number.

# We changed the 'estimator' of RFE (a supervised learning estimator with a fit method that provides information about feature importance) to be aligned with the classifier we used in training process. 
# Eg. for SVM model, we would use SVM in RFE to select features; for KNN model, we would use KNN in RFE as well.

# (reference of RFECV: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html)

# Section 1. Data Sanitization

# Section 1.1. Post-Error Tables (Gives Error-related attributes)


#SQL Environment Setting
import pandas as pd
import sqlite3

# Library Import and Environment Setting needed for Logistic Regression

import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt 
plt.rc("font", size=14)
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import train_test_split
import seaborn as sns
sns.set(style="white")
sns.set(style="whitegrid", color_codes=True)

# Model Environment Setting
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt  
%matplotlib inline 
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV, RFE
from sklearn import metrics
from sklearn.model_selection import train_test_split

from sklearn.utils import resample
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

from sklearn import preprocessing, cross_validation, neighbors, svm
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.svm import SVC
from xgboost.sklearn import XGBClassifier

import pickle

# Make connection to local sqlite database files
sqlite_file = "../sqlite_database/sql/capstone"
conn = sqlite3.connect(sqlite_file)
c = conn.cursor()

# Call 1 - Calling ErrorCauseReason table

# We do not merge ErrorCauseReason table because the columns of this table already exist in ProjectInput and the columns
# that do not exist, are just strings, ie comments.

table_ECR = pd.read_sql_query("SELECT * from 'ErrorCauseReason'",conn)

# Call 2 - Calling ErrorCause table

# We do not merge ErrorCause table because the columns of this table already exist in ProjectInput and the columns
# that do not exist, are just strings, ie comments.

table_ECause = pd.read_sql_query("SELECT * from 'ErrorCause'",conn)

# Call 3 - Calling ErrorCategory table

# We do not merge ErrorCategory table because the columns of this table already exist in ProjectInput and the columns
# that do not exist, are just strings, ie comments.

table_ECategory = pd.read_sql_query("SELECT * from 'ErrorCategory'",conn)

# Call 4 - Calling ErrorScope table

# We do not merge ErrorScope table because the columns of this table already exist in ProjectInput and the columns
# that do not exist, are just strings, ie comments.

table_ES = pd.read_sql_query("SELECT * from 'ErrorScope'",conn)

# Call 5 - Calling ProjectInput table

table_PInput = pd.read_sql_query("SELECT * from 'ProjectInput'",conn)

# We do not merge ProjectInput table because the attributes of error-related rows are so-called "cheater" columns 
# in the following analysis

# Call 6 - Calling ProjectInputAccountability table
# This would be the most important table among all the post-error-occured tables since we need it to label the error-occured rows


table_PIA = pd.read_sql_query("SELECT * from 'ProjectInputAccountability'",conn)

#Label with an added column called "ErrorOccured", 1 = Yes 0 = No.
#Creating a new column error occured with all values as 1
table_PIA = table_PIA.assign(ErrorOccured=1)

# Selecting useful columns in ProjectInputAccountability table
selected_columns = [
    'ProcessInstanceAppianID',
    'ErrorOccured',
]


# No "month columns" in ProjectInputAccountability table
columns_month = [ ]

# No "non_categorical columns" in ProjectInputAccountability table
non_categorical = [ ]


table_PIA = table_PIA[selected_columns]

# Join 1 - Only include table_PIA in this end and not joing PIA and PI because the attributes are post-error related

result_error_related = table_PIA
result_error_related.head()
result_error_related.shape

# Section 1.2. Attribute Tables (Include error or non-error processinstance)

# Call 7 - Calling ProcessStream table

table_PS = pd.read_sql_query("SELECT * from 'ProcessStream'",conn)

# Selecting useful columns in ProcessStream table

selected_columns = [
    'KeyProcessStream', 
    'KeyProcessModel', 
    'ProcessPriority', 
    'ExternalSLA', 
    'SLAAtRiskInterval', 
    'ProcessDurationTarget', 
    'TargetRiskPriorityBasis',
    'TargetPriorityBasis', 
    'SLARiskPriorityBasis', 
    'SLAPriorityBasis', 
    'ExpectedProcessEffort',
    'SameAssigneeNextTask', 
    'KeyAnalystType',
]

# There is not KeyDoc in ProcessStream table but we decided to drop it!

# No "month columns" in ProcessStream table

columns_month = [ ]

# No "non_categorical columns" in ProcessStream table

non_categorical = [ 'ProcessPriority', 
                   'SLAAtRiskInterval', 
                   'ProcessDurationTarget', 
                   'TargetRiskPriorityBasis', 
                   'TargetPriorityBasis', 
                   'SLARiskPriorityBasis', 
                   'SLAPriorityBasis' ]


table_PS = table_PS[selected_columns]

# Call 8 - Calling ProcessInstance table

table_PI = pd.read_sql_query("SELECT * from 'ProcessInstance'",conn)

# Selecting useful columns in ProcessInstance table
selected_columns = [
    'KeyProcessInstance', 
    'ProcessInstanceAppianID',
    'KeyProcessModel', 
    'ProcessInstanceInstantiated',
    'KeyProcessStream',
    'ProcessDurationTarget', 
    'KeyDepartmentEntered'
]


columns_month = [ 
    'ProcessInstanceInstantiated'
]

non_categorical = [ 
    'ProcessInstanceDuration', 
    'ProcessInstanceEffort'
]

table_PI = table_PI[selected_columns]

for i in columns_month:
    table_PI[i+'_month'] = pd.DatetimeIndex(table_PI[i]).month 
    table_PI = table_PI.drop(columns =[i],axis =1)

# Join 2 - Joining ProcessStream with ProcessInstance on KeyProcessStream (key), how = 'left'

result_PI_related = table_PI.merge(table_PS,on='KeyProcessStream',how='left')
result_PI_related.head()

# Call 9 - Calling Departments table

table_Dep = pd.read_sql_query("SELECT * from 'Departments'",conn)

# # Selecting useful columns in Departments table

selected_columns = [
    'KeyDepartment', 
    'KeyPersonHead'
]

# No "month columns" in Departments table

columns_month = [ ]

# No "non_categorical columns" in Departments table

non_categorical = [ ]

table_Dep = table_Dep[selected_columns]

# Renaming KeyDepartment as KeyDepartmentEntered because we ProcessInstance has KeyDepartment as KeyDepartmentEntered
# and we are merging on KeyDepartmentEntered following.

table_Dep = table_Dep.rename(columns = {'KeyDepartment': 'KeyDepartmentEntered'})

# Join 3 - Joining Departments with ProcessInstance on KeyDepartmentEntered (key), how = 'left'

result_PI_related = result_PI_related.merge(table_Dep,on='KeyDepartmentEntered',how='left')
result_PI_related.head()

# Join 4 - Joining result_PI_related with result_error_related on ProcessInstanceAppianID (key), how = 'left'

result = result_PI_related.merge(result_error_related,on='ProcessInstanceAppianID',how='left')
result.head()

# Dropping duplicates columns in result table

columns_duplicates = [ 
    'KeyProcessModel_y', 
    'ProcessDurationTarget_y'
]

for i in columns_duplicates:
    result = result.drop(columns =[i],axis =1)


# Renaming KeyProcessModel_x, ProcessDurationTarget_x, KeyProcessInstance_x as KeyProcessModel, ProcessDurationTarget, 
# KeyProcessInstance because they are duplicates from the join.

result = result.rename(columns = 
                       {
                        'KeyProcessModel_x': 'KeyProcessModel', 
                        'ProcessDurationTarget_x': 'ProcessDurationTarget'
                       })
result.head()


# Dropping KeyProcessInstance because we are using ProcessInstanceAppianID as our index

result = result.drop(columns =['KeyProcessInstance'],axis =1)
result.head()

# Setting ProcessInstanceAppianID as our index

result = result.set_index('ProcessInstanceAppianID')
result.head()

result.shape

# Indicating the columns we use in the analysis
print(result.columns)

# Creating a list indicating the columns that need dummy variables

# Identifying and setting a list with all our categorical columns


non_categorical = [
    'ProcessDurationTarget',  
    'ExpectedProcessEffort',  
    'ProcessPriority'     
]


dummy_col = result.columns.tolist()
for i in non_categorical:
    dummy_col.remove(i)
dummy_col.remove('ErrorOccured')
print (dummy_col)
print (len(non_categorical)+len(dummy_col))

# We drop the NaN values that exist in the ordinal columns 'ProcessDurationTarget', 'ExpectedProcessEffort'

result.dropna(subset=[
    'ProcessDurationTarget',
    'ExpectedProcessEffort'
])

# Creating dummy variables

result_dummies = pd.get_dummies(data=result, columns=dummy_col, dummy_na=False)
result_dummies.head()

# Replacing NaNs in ErrorOccured with 0s

result_dummies['ErrorOccured'] = result_dummies['ErrorOccured'].fillna(0)

for i in non_categorical:
    print(i,result_dummies[i].nunique())

# Dropping the categorical columns that have NaNs 

result_dummies = result_dummies.dropna()
result_dummies.shape

print(result_dummies.columns)

# Section 2. Data Preprocessing

df = result_dummies
print('Dimension of dataset:', df.shape)

# Split Predictors and outcome variable
y = df.loc[:, ['ErrorOccured']]
X = df.loc[:, df.columns != 'ErrorOccured']

print('Dimension of predictors X:', X.shape)
print('Dimension of outcome y:', y.shape)
print()
print('Error occurence rate: '+'{:.4%}'.format(np.sum(y.values)/len(y)))

# Section 2.1. Oversampling 

# This is the counting of error and non error rows in the initial dataset
unique, counts = np.unique(y, return_counts=True)
print()
print('This is the counting of error and non error rows in the initial dataset:')
print(dict(zip(unique, counts)))

# Separate majority and minority classes

df_minority_error = df[df['ErrorOccured'] == 1]
df_majority_nonerror = df[df['ErrorOccured'] == 0]

# Upsample minority class

df_minority_upsampled = resample(df_minority_error, 
                                  replace=True,     # sample with replacement
                                  n_samples=82206,    # to match majority class
                                  random_state=123) # reproducible results

# Combine majority class with upsampled minority class
df_upsampled = pd.concat([df_majority_nonerror, df_minority_upsampled])
 
# Display new class counts
df_upsampled.ErrorOccured.value_counts()

# Separate input features (X) and target variable (y)
y_new = df_upsampled.ErrorOccured
X_new = df_upsampled.drop('ErrorOccured', axis=1)
 
# Train model
clf_1 = LogisticRegression().fit(X_new, y_new)
 
# Predict on training set
pred_y_1 = clf_1.predict(X_new)
 
# Is our model still predicting just one class?
print( np.unique( pred_y_1 ) )
 
# How's our accuracy?
print( accuracy_score(y_new, pred_y_1) )

# Section 3. Model Selection

# 3.1 Logistic Regression 

%%time

### 1) FEATURE SELECTION

from sklearn.linear_model import LogisticRegression

# RFE Feature Selection using Logistic Regression                     
estimator = LogisticRegression()                       # we used logistic regression here for RFE estimator
rfe = RFECV(estimator, step=1, cv=5)                   # step - number of features dropped each time in RFE
rfe = rfe.fit(X_new, y_new)

# Print number of features selected
selected_features_logreg = X_new.columns[rfe.support_.nonzero()].tolist()
print('Number of features selected:', rfe.n_features_)
print("The selected features are: ")
print()
print(selected_features_logreg)

# Keep only the selected features in our dataset
X_selected_logreg = X_new[selected_features_logreg]

# Split data into train set and test set
X_train_logreg, X_test_logreg, y_train_logreg, y_test_logreg = train_test_split(X_selected_logreg, y_new, test_size = 0.3, random_state=123)

# Check data shape
print()
print('Shape of Train/Test Dataset:')
print()
print('X_train:', X_train_logreg.shape)
print('y_train:', y_train_logreg.shape)
print('X_test:', X_test_logreg.shape)
print('y_test:', y_test_logreg.shape)

%%time

### 2) TRAIN AND TEST MODEL WITH SELECTED FEATURES

# Train on training data
logreg = LogisticRegression().fit(X_train_logreg, y_train_logreg)

# Test on testing data
y_pred_logreg = logreg.predict(X_test_logreg)
logreg_score = logreg.score(X_test_logreg, y_test_logreg)
print('Accuracy of Logistic Regression classifier: {:.3%}'.format(logreg_score))
print(metrics.classification_report(y_test_logreg, y_pred_logreg))

# Plot confusion matrix
labels = [0,1]
cm = confusion_matrix(y_test_logreg, y_pred_logreg, labels)
print()
print('Confusion Matrix: ')
print('pred:', 0, '   ', 1)
print(cm)

# Plot ROC curve
logreg_roc_auc = roc_auc_score(y_test_logreg, y_pred_logreg)
fpr, tpr, thresholds = roc_curve(y_test_logreg, logreg.predict_proba(X_test_logreg)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logreg_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating haracteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

# Section 3.2. K Nearest Neighbors

%%time

### 1) TRAIN-TEST DATA SPLIT WITH ALL FEATURES

# Split data into train set and test set
X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_new, y_new, test_size = 0.3, random_state=123, stratify = y_new)


# Check data shape
print('Shape of Train/Test Dataset:')
print()
print('X_train:', X_train_knn.shape)
print('y_train:', y_train_knn.shape)
print('X_test:', X_test_knn.shape)
print('y_test:', y_test_knn.shape)

%%time

### 2) TRAIN AND TEST MODEL WITH ALL FEATURES

from sklearn import preprocessing, neighbors

# Train
knn = neighbors.KNeighborsClassifier()
knn.fit(X_train_knn, y_train_knn)

# Test
y_pred_knn = knn.predict(X_test_knn)
knn_score = knn.score(X_test_knn, y_test_knn)
print('Accuracy of KNN classifier: {:.3%}'.format(knn_score))
knn_auc = roc_auc_score(y_test_knn, y_pred_knn)
print(metrics.classification_report(y_test_knn, y_pred_knn))

# Plot confusion matrix
labels = [0,1]
cm = confusion_matrix(y_test_knn, y_pred_knn, labels)
print()
print('Confusion Matrix: ')
print('pred:', 0, '   ', 1)
print(cm)

# plot ROC curve
knn_auc = roc_auc_score(y_test_knn, y_pred_knn)
fpr, tpr, thresholds = roc_curve(y_test_knn, knn.predict_proba(X_test_knn)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='K Nearest Neighbors model (area = %0.2f)' % knn_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

# Section 3.3. Support Vector Machine

# LinearSVC( ) does not work for RFE estimator, so we imported all the features in SVM model to see the test result

%%time

# Split data into train set and test set
X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_new, y_new, test_size = 0.3, random_state=123)

# Check data shape
print('Shape of Train/Test Dataset:')
print()
print('X_train:', X_train_svm.shape)
print('y_train:', y_train_svm.shape)
print('X_test:', X_test_svm.shape)
print('y_test:', y_test_svm.shape)

%%time

### 2) TRAIN AND TEST MODEL WITH SELECTED FEATURES

from sklearn import svm

# Train
svm = svm.LinearSVC()  
svm.fit(X_train_svm, y_train_svm)

# Test
y_pred_svm = svm.predict(X_test_svm)
svm_score = svm.score(X_test_svm, y_test_svm)
print('Accuracy of SVM classifier: {:.3%}'.format(svm_score))
print(metrics.classification_report(y_test_svm, y_pred_svm))

# Plot confusion matrix
labels = [0,1]
cm = confusion_matrix(y_test_svm, y_pred_svm, labels)
print()
print('Confusion Matrix: ')
print('pred:', 0, '   ', 1)
print(cm)

# Section 3.4. Decision Tree

# For decision tree, we used cross validation to pick the best performed number of layers instead of features to prevent over fitting.

%%time

### 1) LAYER SELECTION

from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits = 5)
skf.get_n_splits(X_new, y_new)

selection = dict()
for k in range(1,11):
    selection[k] = list()
    for train_index, test_index in skf.split(X_new, y_new):
        X_train_dtree, X_test_dtree = X_new.iloc[train_index], X_new.iloc[test_index]
        y_train_dtree, y_test_dtree = y_new.iloc[train_index], y_new.iloc[test_index]
        dtree = tree.DecisionTreeClassifier(max_depth = k, criterion = 'entropy')
        dtree.fit(X_train_dtree,y_train_dtree)
        p_test = dtree.predict(X_test_dtree)
        selection[k].append(metrics.accuracy_score(p_test, y_test_dtree))
    selection[k] = np.mean(selection[k])

bestK = max(selection.items(), key = lambda x: x[1])[0]
print('Selected Number of Layers: ', bestK)

%%time

### 2) PLOT DECISION TREE WITH SELECTED NUMBER OF LAYERS

!pip install pydotplus        # uncomment to install package "pydotplus" if not found
import pydotplus 
from IPython.display import Image

# Split data into train set and test set
X_train_dtree, X_test_dtree, y_train_dtree, y_test_dtree = train_test_split(X_new, y_new, test_size = 0.3, random_state=123)

# Train
dtree = tree.DecisionTreeClassifier(max_depth = bestK, criterion = 'entropy')
dtree.fit(X_new,y_new)
feature_names = X_new.keys()

# plot decition tree 
dot_data = tree.export_graphviz(dtree, out_file=None, feature_names=feature_names) 
graph = pydotplus.graphviz.graph_from_dot_data(dot_data)
Image(graph.create_png())

%%time

### 3) TEST ACCURACY OF DECISION TREE

# Test
y_pred_dtree = dtree.predict(X_test_dtree)
dtree_score = dtree.score(X_test_dtree, y_test_dtree)
print('Accuracy of Decision Tree classifier: {:.3%}'.format(dtree_score))
print(metrics.classification_report(y_test_dtree, y_pred_dtree))

# Plot confusion matrix
labels = [0,1]
cm = confusion_matrix(y_test_dtree, y_pred_dtree, labels)
print()
print('Confusion Matrix: ')
print('pred:', 0, '   ', 1)
print(cm)

# plot ROC curve
dtree_auc = roc_auc_score(y_test_dtree, y_pred_dtree)
fpr, tpr, thresholds = roc_curve(y_test_dtree, dtree.predict_proba(X_test_dtree)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Decision Tree model (area = %0.2f)' % dtree_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

# Section 3.5. Random Forest

%%time

### 1) FEATURE SELECTION

# RFE Feature Selection using Random Forest                      
estimator = RandomForestClassifier(max_features = "log2", max_depth = 10)         # we change the estimator for random forest classifier
rfe = RFECV(estimator, step = 1, cv = 5)    
rfe = rfe.fit(X_new, y_new)

# Print number of features selected
selected_features_rf = X_new.columns[rfe.support_.nonzero()].tolist()
print('Number of features selected:', rfe.n_features_)
print("The selected features are: ")
print()
print(selected_features_rf)

# Keep only the selected features in our dataset
X_selected_rf = X_new[selected_features_rf]

# Split data into train set and test set
X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_selected_rf, y_new, test_size = 0.3, random_state=123)

# Check data shape
print('Shape of Train/Test Dataset:')
print()
print('X_train:', X_train_rf.shape)
print('y_train:', y_train_rf.shape)
print('X_test:', X_test_rf.shape)
print('y_test:', y_test_rf.shape)

%%time

### 2) TRAIN AND TEST MODEL WITH SELECTED FEATURES

# Train
rf = RandomForestClassifier()
rf.fit(X_train_rf, y_train_rf)  

# Test
y_pred_rf = rf.predict(X_test_rf)
rf_score = rf.score(X_test_rf, y_test_rf)
print('Accuracy of Random Forest classifier: {:.3%}'.format(rf_score))
print(metrics.classification_report(y_test_rf, y_pred_rf))


# Plot confusion matrix
labels = [0,1]
cm = confusion_matrix(y_test_rf, y_pred_rf, labels)
print()
print('Confusion Matrix: ')
print('pred:', 0, '   ', 1)
print(cm)

# plot ROC curve
rf_auc = roc_auc_score(y_test_rf, y_pred_rf)
fpr, tpr, thresholds = roc_curve(y_test_rf, rf.predict_proba(X_test_rf)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Random Forest model (area = %0.2f)' % rf_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

Section 3.6. XGBoost Classifier

%%time
from xgboost.sklearn import XGBClassifier

# Split data into train set and test set
X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_new, y_new, test_size = 0.3, random_state=123)

# Check data shape
print('Shape of Train/Test Dataset:')
print()
print('X_train:', X_train_xgb.shape)
print('y_train:', y_train_xgb.shape)
print('X_test:', X_test_xgb.shape)
print('y_test:', y_test_xgb.shape)

### 2) TRAIN AND TEST MODEL WITH SELECTED FEATURES

# Train
xgb = XGBClassifier()   
xgb.fit(X_train_xgb, y_train_xgb)  

# Test
y_pred_xgb = xgb.predict(X_test_xgb)
xgb_score = xgb.score(X_test_xgb, y_test_xgb)
print('Accuracy of XGBoost classifier: {:.3%}'.format(xgb_score))
print(metrics.classification_report(y_test_xgb, y_pred_xgb))


# Plot confusion matrix
labels = [0,1]
cm = confusion_matrix(y_test_xgb, y_pred_xgb, labels)
print()
print('Confusion Matrix: ')
print('pred:', 0, '   ', 1)
print(cm)

# plot ROC curve
xgb_auc = roc_auc_score(y_test_xgb, y_pred_xgb)
fpr, tpr, thresholds = roc_curve(y_test_xgb, xgb.predict_proba(X_test_xgb)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='XGBoost Classifier (area = %0.2f)' % xgb_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

# Section 4. Serialization

# We create pickle files for all the model we trained above, for your future use in case.

# Serialize each model

## Logistic Regression Model

with open('Logistic_Regression.pkl', 'wb') as f1:
    pickle.dump(logreg, f1)

## Random Forest Classifier

with open('Random_Forest_Classifier.pkl', 'wb') as f2:
    pickle.dump(rf, f2)
    
## K Nearest Neighbors Classifier
    
with open('K_Nearest_Neighbors_Classifier.pkl', 'wb') as f3:
    pickle.dump(knn, f3)
    
## Support Vector Machine Classifier

with open('Support_Vector_Machine_Classifier.pkl', 'wb') as f4:
    pickle.dump(svm, f4)
    
## Decision Tree Classifier

with open('Decision_Tree_Classifier.pkl', 'wb') as f5:
    pickle.dump(dtree, f5)
    
## XGBoost Classifier

with open('XGBoost_Classifier.pkl', 'wb') as f6:
    pickle.dump(xgb, f6)

with open('Random_Forest_Classifier.pkl', 'wb') as f2:
    pickle.dump(rf, f2)
with open('Random_Forest_Classifier.pkl', 'rb') as f2:
    rf_model = pickle.load(f2)

# Printing out the feature importances

%matplotlib inline
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.xlabel('Relative Importance')
indices = np.argsort(importances)
dummy = plt.yticks(range(len(indices)),selected_features_rf) 
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
dummy = plt.yticks(range(len(indices)),indices)

# Doing the matching between attribute name and attributes' ranking 

print("Feature ranking:")
for i in range(1,len(selected_features_rf)+1):
    print(i, list(sorted(zip(importances,selected_features_rf), key = lambda x: x[0], reverse=True))[i-1])

# Section 5. Validation Data Preprocessing

# We pick Random Forest classifier as our final model due to its high accuracy, high precision and recal and short training time. Therefore, we only do validation data preprocessing and inference specifically for random forest model. It's also worthwile to play with other models since we already create pickle files for each model.

# Read validation data
valdata = pd.read_csv('../data/dec_2017_and_jan_2019_tasks_all_features_columbia_students_used_without_so_called_cheating_features.csv')

# Check shape and columns
print(valdata.shape)
print(valdata.columns)
valdata.head()

# Rename columns to be consistent with training set
valdf = valdata.rename(columns = 
                       {
                           'processinstanceinstantiated': 'ProcessInstanceInstantiated',
                           'keyprocessstream': 'KeyProcessStream',
                           'keydepartmententered': 'KeyDepartmentEntered',
                           'KeyPersonHead of Department which entered the work': 'KeyPersonHead',
                           'externalsla': 'ExternalSLA',
                           'slaatriskinterval': 'SLAAtRiskInterval',
                           'targetriskprioritybasis': 'TargetRiskPriorityBasis',
                           'targetprioritybasis': 'TargetPriorityBasis',
                           'slariskprioritybasis': 'SLARiskPriorityBasis',
                           'slaprioritybasis': 'SLAPriorityBasis',
                           'processdurationtarget': 'ProcessDurationTarget',
                           'processpriority': 'ProcessPriority',
                           'expectedprocesseffort': 'ExpectedProcessEffort'
                       })

# Drop useless data info
valdf['ProcessInstanceInstantiated_month'] = pd.DatetimeIndex(valdf['ProcessInstanceInstantiated']).month
valdf = valdf.drop(columns =['ProcessInstanceInstantiated',
                             'KeyPersonHead of KeyDepartment 190',
                             'sameassigneenexttask'],axis =1)

# Delete commas in columns
valdf = valdf.astype('str')
for i in valdf.columns:
    valdf[i] = valdf[i].str.replace(',','')
    
# Drop nan in non-categorical columns
valdf = valdf.drop(valdf[(valdf['ProcessDurationTarget'] == 'nan')].index)

print(valdf.shape)
valdf.head()

# Create dummy variables for categorical variables and check the shape afterwards
need_dummy_col = valdf.columns.tolist()
for i in non_categorical:
    need_dummy_col.remove(i)
    valdf[i] = valdf[i].astype('float')
valdf['KeyPersonHead'] = valdf['KeyPersonHead'].astype('float')

valdata_dm = pd.get_dummies(data = valdf, columns = need_dummy_col, dummy_na = False)
print(valdata_dm.shape)
#print(valdata_dm.columns)   # uncomment to print all the dummy variables

# Print all the features trained in Random Forest classifier
print(f'The number of features trained in Random Forest classifer: {len(selected_features_rf)}')
print()
print(selected_features_rf)

# Drop columns not selected in the features selection before
new_columns = valdata_dm.columns.tolist()
for i in new_columns:
    if i not in selected_features_rf:
        valdata_dm.drop(columns = [i], axis = 1, inplace = True)

# Add zero columns that needed for other needed features
for i in selected_features_rf:
    if i not in new_columns:
        valdata_dm[i] = 0

# change the order of features to be consistent with training
valdata_dm = valdata_dm[X_train_rf.columns]

# check the final data
print(valdata_dm.shape)
print(np.sum(valdata_dm))         # uncomment to calculate the number of 1s in each column of the validation data
valdata_dm.head()

# Section 6. Inference

# Section 6.1. Deserialization for Random Forest Classifier

with open('Random_Forest_Classifier.pkl', 'rb') as f2:
    rf_model = pickle.load(f2)

# Section 6.2. Run Inference and Export Results

# Validate Random Forest model
prediction = rf_model.predict(valdata_dm)
nonan = valdata[valdata['processdurationtarget'].isnull()!=True]
nan = valdata[valdata['processdurationtarget'].isnull()==True]
nonan['prediction'] = prediction
nan['prediction'] = np.NaN
results_rf = pd.concat([nonan,nan])
print('output data shape:', results_rf.shape)
print('check number of 1s in prediction column:', np.sum(results_rf['prediction']))

# Write to CSV
results_rf.to_csv('inference_results_from_Random_Forest_for_sp.csv')
